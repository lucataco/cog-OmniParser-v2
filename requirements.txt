torch==2.4.0
torchvision
easyocr==1.7.1
supervision==0.18.0
transformers==4.49.0
ultralytics==8.3.70
numpy<2
einops==0.8.0
opencv-python-headless==4.11.0.86
pillow
timm==1.0.14
accelerate
huggingface-hub
https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.4cxx11abiFALSE-cp312-cp312-linux_x86_64.whl